# -*- coding: utf-8 -*-
"""customcnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/120IJDjifFNYb0u36MboBMLt9DLLu7LkV

# Klasifikasi Kardiomegali Menggunakan CNN
"""

from google.colab import files
files.upload()

!pip install -q kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/kaggle.json
!chmod 600 ~/.kaggle/kaggle.json

# Download dataset
!kaggle datasets download -d cxhn5201/chestray83
!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia

# Ekstrak dataset
!unzip -q chestray83.zip 'Cardiomegaly/*' -d data_raw
!unzip -q chest-xray-pneumonia.zip 'chest_xray/train/NORMAL/*' -d Normal_Xray

import os

# Define the paths to the directories
cardiomegaly_dir = 'data_raw/Cardiomegaly'
normal_dir = 'Normal_Xray/chest_xray/train/NORMAL'

# Count the number of files in each directory
cardiomegaly_count = len([name for name in os.listdir(cardiomegaly_dir) if os.path.isfile(os.path.join(cardiomegaly_dir, name))])
normal_count = len([name for name in os.listdir(normal_dir) if os.path.isfile(os.path.join(normal_dir, name))])

# Print the counts
print(f"Number of Cardiomegaly X-rays: {cardiomegaly_count}")
print(f"Number of Normal X-rays: {normal_count}")

# Compare the counts
if cardiomegaly_count > normal_count:
  print("There are more Cardiomegaly X-rays than Normal X-rays.")
elif cardiomegaly_count < normal_count:
  print("There are more Normal X-rays than Cardiomegaly X-rays.")
else:
  print("The number of Cardiomegaly X-rays and Normal X-rays are equal.")

import os, shutil

os.makedirs('data_raw/Normal', exist_ok=True)
src_dir = 'Normal_Xray/chest_xray/train/NORMAL'

for fname in os.listdir(src_dir):
    shutil.copy(os.path.join(src_dir, fname), os.path.join('data_raw/Normal', fname))

import random
from pathlib import Path

# Check if the 'data_raw/Normal' directory exists, create it if not
if not os.path.exists('data_raw/Normal'):
    print("Warning: 'data_raw/Normal' directory not found. Attempting to extract again...")
    !unzip -q chestray83.zip -d data_raw  # Re-extract in case it was missed
    if not os.path.exists('data_raw/Normal'):
        raise FileNotFoundError("'data_raw/Normal' directory not found even after re-extraction. Please check the dataset.")


os.makedirs('data/train/Cardiomegaly', exist_ok=True)
os.makedirs('data/train/Normal', exist_ok=True)
os.makedirs('data/val/Cardiomegaly', exist_ok=True)
os.makedirs('data/val/Normal', exist_ok=True)
os.makedirs('data/test/Cardiomegaly', exist_ok=True)
os.makedirs('data/test/Normal', exist_ok=True)

def bagi_data(src_folder, dst_folder, ratio=(0.7, 0.15, 0.15)):
    files = os.listdir(src_folder)
    random.shuffle(files)
    total = len(files)
    train_end = int(ratio[0] * total)
    val_end = train_end + int(ratio[1] * total)

    for i, f in enumerate(files):
        src = os.path.join(src_folder, f)
        if i < train_end:
            dst = os.path.join(dst_folder, 'train', Path(src_folder).name, f)
        elif i < val_end:
            dst = os.path.join(dst_folder, 'val', Path(src_folder).name, f)
        else:
            dst = os.path.join(dst_folder, 'test', Path(src_folder).name, f)
        shutil.copy(src, dst)

# Bagi dataset
bagi_data('data_raw/Cardiomegaly', 'data')
bagi_data('data_raw/Normal', 'data')

# Import libraries
import os
import cv2
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix, classification_report

# Load dataset
labels = ['Cardiomegaly', 'Normal']
img_size = 70

def get_data(data_dir):
    data = []
    for label in labels:
        path = os.path.join(data_dir, label)
        class_num = labels.index(label)
        for img in os.listdir(path):
            try:
                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)
                resized_arr = cv2.resize(img_arr, (img_size, img_size))
                data.append([resized_arr, class_num])
            except Exception as e:
                pass
    return np.array(data)

import cv2
import numpy as np
import os

img_size = 224  # Ubah sesuai kebutuhanmu

def get_data(data_dir):
    data = []
    categories = os.listdir(data_dir)

    for category in categories:
        category_path = os.path.join(data_dir, category)
        label = categories.index(category)

        for img_file in os.listdir(category_path):
            img_path = os.path.join(category_path, img_file)
            try:
                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
                if img is not None:
                    img = cv2.resize(img, (img_size, img_size))
                    data.append((img, label))
                else:
                    print(f"[Warning] Gagal membaca: {img_path}")
            except Exception as e:
                print(f"[Error] {img_path} â€” {e}")
                continue

    return data

# Load datasets
train = get_data("data/train")
val = get_data("data/val")
test = get_data("data/test")

# Split fitur dan label
x_train, y_train = zip(*train)
x_val, y_val = zip(*val)
x_test, y_test = zip(*test)

# Normalisasi dan reshape
x_train = np.array(x_train).reshape(-1, img_size, img_size, 1) / 255.0
x_val = np.array(x_val).reshape(-1, img_size, img_size, 1) / 255.0
x_test = np.array(x_test).reshape(-1, img_size, img_size, 1) / 255.0

y_train = np.array(y_train)
y_val = np.array(y_val)
y_test = np.array(y_test)

import matplotlib.pyplot as plt
import os
import random
from PIL import Image

# Path folder
cardio_path = '/content/data_raw/Cardiomegaly'
normal_path = '/content/data_raw/Normal'

# Ambil 5 gambar dari masing-masing kelas
cardio_images = random.sample(os.listdir(cardio_path), 5)
normal_images = random.sample(os.listdir(normal_path), 5)

# Gabungkan dan beri label
all_images = [(os.path.join(cardio_path, img), 'Cardiomegaly') for img in cardio_images] + \
             [(os.path.join(normal_path, img), 'Normal') for img in normal_images]

# Acak urutan
random.shuffle(all_images)

# Tampilkan gambar dalam grid 2x5
plt.figure(figsize=(15, 6))
for i, (img_path, label) in enumerate(all_images):
    img = Image.open(img_path).resize((224, 224))
    plt.subplot(2, 5, i + 1)
    plt.imshow(img, cmap='gray')
    plt.title(label, fontsize=10)
    plt.axis('off')
plt.tight_layout()
plt.show()

# Data Augmentation
datagen = ImageDataGenerator(
    rotation_range=30,
    zoom_range=0.2,
    width_shift_range=0.1,
    height_shift_range=0.1
)
datagen.fit(x_train)

# CNN Model
model = Sequential([
    Conv2D(32, (3,3), activation='relu', padding='same', input_shape=(img_size, img_size, 1)),
    MaxPooling2D(pool_size=(2,2), padding='same'),
    Conv2D(32, (3,3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2,2), padding='same'),
    Conv2D(64, (3,3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2,2), padding='same'),
    Conv2D(128, (3,3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2,2), padding='same'),
    Conv2D(256, (3,3), activation='relu', padding='same'),
    MaxPooling2D(pool_size=(2,2), padding='same'),
    Flatten(),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

# Train the model
history = model.fit(datagen.flow(x_train, y_train, batch_size=50), epochs=20, validation_data=(x_val, y_val))

# Evaluation
loss, accuracy = model.evaluate(x_test, y_test)
print(f"Accuracy on test data: {accuracy*100:.2f}%")
print(f"Loss on test data: {loss*100:.2f}%")

# Confusion Matrix
y_pred = model.predict(x_test)
y_pred = (y_pred > 0.5).astype("int32")
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

fig, ax = plt.subplots(1, 2, figsize=(6.22, 3.11))

# Akurasi
ax[0].plot(history.history['accuracy'], label='Train')
ax[0].plot(history.history['val_accuracy'], label='Val')
ax[0].set_title('Accuracy'); ax[0].legend(); ax[0].grid(True)

# Loss
ax[1].plot(history.history['loss'], label='Train')
ax[1].plot(history.history['val_loss'], label='Val')
ax[1].set_title('Loss'); ax[1].legend(); ax[1].grid(True)

plt.tight_layout()
plt.show()

model.save('model.h5')

from tensorflow.keras.models import load_model
import numpy as np
import cv2

# Load the saved model
model = load_model('model.h5')

# Example prediction (replace with your actual image data)
# Preprocess the image (resize, normalize, reshape) as done during training
# Example:
example_image = cv2.imread('/content/data/test/Normal/NORMAL2-IM-1120-0001.jpeg', cv2.IMREAD_GRAYSCALE)  # Replace with your image path
example_image = cv2.resize(example_image, (224, 224))  # Assuming 224x224 input size
example_image = example_image.reshape(-1, 224, 224, 1) / 255.0

# Make prediction
prediction = model.predict(example_image)

# Interpret the prediction (assuming binary classification)
predicted_class = "Cardiomegaly" if prediction[0][0] > 0.5 else "Normal"
confidence_score = prediction[0][0] if prediction[0][0] > 0.5 else 1 - prediction[0][0]

print(f"Predicted class: {predicted_class}")
print(f"Prediction score: {confidence_score:.4f}")